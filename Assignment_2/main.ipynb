{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94ea2434",
   "metadata": {},
   "source": [
    "# Logistic Regression: Implement signle layer perceptron for glass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddf928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Glass Classification - Single Layer Perceptron (PyTorch)\n",
      "======================================================================\n",
      "\n",
      "Dataset: 214 samples, 9 features\n",
      "\n",
      "Class counts:\n",
      "Type\n",
      "1    70\n",
      "2    76\n",
      "3    17\n",
      "5    13\n",
      "6     9\n",
      "7    29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Features: (214, 9), Classes: 6\n",
      "Label mapping: {np.int64(1): 0, np.int64(2): 1, np.int64(3): 2, np.int64(5): 3, np.int64(6): 4, np.int64(7): 5}\n",
      "Train: 149, Test: 65\n",
      "\n",
      "======================================================================\n",
      "Architecture\n",
      "======================================================================\n",
      "SLP(\n",
      "  (layer): Linear(in_features=9, out_features=6, bias=True)\n",
      ")\n",
      "\n",
      "Params: 60\n",
      "\n",
      "======================================================================\n",
      "Training\n",
      "======================================================================\n",
      "Epoch   10 | Loss: 0.9879 | Train: 0.6107 | Test: 0.6923\n",
      "Epoch   20 | Loss: 0.9172 | Train: 0.6376 | Test: 0.7538\n",
      "Epoch   30 | Loss: 0.8675 | Train: 0.6309 | Test: 0.7385\n",
      "Epoch   40 | Loss: 0.8022 | Train: 0.6510 | Test: 0.7385\n",
      "Epoch   50 | Loss: 0.8336 | Train: 0.6510 | Test: 0.7385\n",
      "Epoch   60 | Loss: 0.7757 | Train: 0.6376 | Test: 0.7385\n",
      "Epoch   70 | Loss: 0.7960 | Train: 0.6711 | Test: 0.7385\n",
      "Epoch   80 | Loss: 0.8150 | Train: 0.6577 | Test: 0.7385\n",
      "Epoch   90 | Loss: 0.7786 | Train: 0.6711 | Test: 0.7385\n",
      "Epoch  100 | Loss: 0.7604 | Train: 0.6644 | Test: 0.7077\n",
      "Epoch  110 | Loss: 0.7620 | Train: 0.6577 | Test: 0.7231\n",
      "Epoch  120 | Loss: 0.7720 | Train: 0.6577 | Test: 0.7231\n",
      "Epoch  130 | Loss: 0.7292 | Train: 0.6846 | Test: 0.7385\n",
      "Epoch  140 | Loss: 0.7394 | Train: 0.6779 | Test: 0.7231\n",
      "Epoch  150 | Loss: 0.7493 | Train: 0.6846 | Test: 0.7231\n",
      "Epoch  160 | Loss: 0.7313 | Train: 0.6711 | Test: 0.7385\n",
      "Epoch  170 | Loss: 0.7308 | Train: 0.6846 | Test: 0.7231\n",
      "Epoch  180 | Loss: 0.7339 | Train: 0.6644 | Test: 0.7231\n",
      "Epoch  190 | Loss: 0.7376 | Train: 0.6577 | Test: 0.7231\n",
      "Epoch  200 | Loss: 0.7424 | Train: 0.6779 | Test: 0.7231\n",
      "Epoch  210 | Loss: 0.7209 | Train: 0.6779 | Test: 0.7385\n",
      "Epoch  220 | Loss: 0.7383 | Train: 0.6711 | Test: 0.7231\n",
      "Epoch  230 | Loss: 0.7248 | Train: 0.6846 | Test: 0.7231\n",
      "Epoch  240 | Loss: 0.7336 | Train: 0.6913 | Test: 0.7385\n",
      "Epoch  250 | Loss: 0.6922 | Train: 0.6846 | Test: 0.7077\n",
      "Epoch  260 | Loss: 0.7071 | Train: 0.6846 | Test: 0.7077\n",
      "Epoch  270 | Loss: 0.7321 | Train: 0.6779 | Test: 0.7231\n",
      "Epoch  280 | Loss: 0.6898 | Train: 0.6779 | Test: 0.7231\n",
      "Epoch  290 | Loss: 0.6991 | Train: 0.6846 | Test: 0.7231\n",
      "Epoch  300 | Loss: 0.6872 | Train: 0.6846 | Test: 0.7231\n",
      "Epoch  310 | Loss: 0.7002 | Train: 0.6711 | Test: 0.7077\n",
      "Epoch  320 | Loss: 0.6880 | Train: 0.6779 | Test: 0.7077\n",
      "Epoch  330 | Loss: 0.7010 | Train: 0.6779 | Test: 0.7077\n",
      "Epoch  340 | Loss: 0.7101 | Train: 0.6779 | Test: 0.7077\n",
      "Epoch  350 | Loss: 0.6901 | Train: 0.6846 | Test: 0.7077\n",
      "Epoch  360 | Loss: 0.7183 | Train: 0.6913 | Test: 0.7077\n",
      "Epoch  370 | Loss: 0.6749 | Train: 0.6846 | Test: 0.7077\n",
      "Epoch  380 | Loss: 0.7191 | Train: 0.6779 | Test: 0.6923\n",
      "Epoch  390 | Loss: 0.6941 | Train: 0.6846 | Test: 0.6923\n",
      "Epoch  400 | Loss: 0.6893 | Train: 0.6846 | Test: 0.6923\n",
      "Epoch  410 | Loss: 0.6961 | Train: 0.6779 | Test: 0.6923\n",
      "Epoch  420 | Loss: 0.7006 | Train: 0.6980 | Test: 0.6923\n",
      "Epoch  430 | Loss: 0.6757 | Train: 0.6846 | Test: 0.6923\n",
      "Epoch  440 | Loss: 0.6589 | Train: 0.6846 | Test: 0.6923\n",
      "Epoch  450 | Loss: 0.6961 | Train: 0.6846 | Test: 0.6923\n",
      "Epoch  460 | Loss: 0.6943 | Train: 0.6846 | Test: 0.6923\n",
      "Epoch  470 | Loss: 0.6791 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  480 | Loss: 0.6876 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  490 | Loss: 0.6885 | Train: 0.6711 | Test: 0.6769\n",
      "Epoch  500 | Loss: 0.6742 | Train: 0.6779 | Test: 0.6769\n",
      "Epoch  510 | Loss: 0.7099 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  520 | Loss: 0.6589 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  530 | Loss: 0.6806 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  540 | Loss: 0.6612 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  550 | Loss: 0.6410 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  560 | Loss: 0.6589 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  570 | Loss: 0.7030 | Train: 0.6779 | Test: 0.6769\n",
      "Epoch  580 | Loss: 0.6626 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  590 | Loss: 0.6824 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  600 | Loss: 0.6715 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  610 | Loss: 0.6685 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  620 | Loss: 0.6500 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  630 | Loss: 0.6767 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  640 | Loss: 0.6384 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  650 | Loss: 0.6514 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  660 | Loss: 0.6449 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  670 | Loss: 0.6697 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  680 | Loss: 0.6359 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  690 | Loss: 0.6545 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  700 | Loss: 0.6488 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  710 | Loss: 0.6395 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  720 | Loss: 0.6585 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  730 | Loss: 0.6880 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  740 | Loss: 0.6525 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  750 | Loss: 0.6497 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  760 | Loss: 0.6800 | Train: 0.7114 | Test: 0.6769\n",
      "Epoch  770 | Loss: 0.6271 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  780 | Loss: 0.6493 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  790 | Loss: 0.6430 | Train: 0.7047 | Test: 0.6769\n",
      "Epoch  800 | Loss: 0.6889 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  810 | Loss: 0.6423 | Train: 0.7047 | Test: 0.6769\n",
      "Epoch  820 | Loss: 0.6842 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  830 | Loss: 0.6558 | Train: 0.6846 | Test: 0.6615\n",
      "Epoch  840 | Loss: 0.6437 | Train: 0.7047 | Test: 0.6615\n",
      "Epoch  850 | Loss: 0.6942 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  860 | Loss: 0.6416 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  870 | Loss: 0.6544 | Train: 0.6846 | Test: 0.6769\n",
      "Epoch  880 | Loss: 0.6301 | Train: 0.7114 | Test: 0.6769\n",
      "Epoch  890 | Loss: 0.6424 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  900 | Loss: 0.6482 | Train: 0.6980 | Test: 0.6615\n",
      "Epoch  910 | Loss: 0.6868 | Train: 0.6913 | Test: 0.6615\n",
      "Epoch  920 | Loss: 0.6460 | Train: 0.7181 | Test: 0.6769\n",
      "Epoch  930 | Loss: 0.6304 | Train: 0.6913 | Test: 0.6769\n",
      "Epoch  940 | Loss: 0.6665 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  950 | Loss: 0.6463 | Train: 0.7047 | Test: 0.6769\n",
      "Epoch  960 | Loss: 0.6227 | Train: 0.7047 | Test: 0.6769\n",
      "Epoch  970 | Loss: 0.6527 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  980 | Loss: 0.6274 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch  990 | Loss: 0.6536 | Train: 0.6980 | Test: 0.6769\n",
      "Epoch 1000 | Loss: 0.6380 | Train: 0.7181 | Test: 0.6769\n",
      "\n",
      "======================================================================\n",
      "Results\n",
      "======================================================================\n",
      "\n",
      "Train acc: 0.7047 (70.47%)\n",
      "Test acc: 0.6769 (67.69%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Report (Test)\n",
      "----------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.76      0.70        21\n",
      "           2       0.64      0.61      0.62        23\n",
      "           3       0.00      0.00      0.00         5\n",
      "           5       0.75      0.75      0.75         4\n",
      "           6       0.60      1.00      0.75         3\n",
      "           7       0.89      0.89      0.89         9\n",
      "\n",
      "    accuracy                           0.68        65\n",
      "   macro avg       0.59      0.67      0.62        65\n",
      "weighted avg       0.63      0.68      0.65        65\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "----------------------------------------------------------------------\n",
      "[[16  5  0  0  0  0]\n",
      " [ 5 14  0  1  2  1]\n",
      " [ 4  1  0  0  0  0]\n",
      " [ 0  1  0  3  0  0]\n",
      " [ 0  0  0  0  3  0]\n",
      " [ 0  1  0  0  0  8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyVirtualEnvs\\Pytorch_NN_RnD\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\MyVirtualEnvs\\Pytorch_NN_RnD\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\MyVirtualEnvs\\Pytorch_NN_RnD\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph saved: metrics_pytorch.png\n",
      "Model saved: glass_model.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(67)\n",
    "np.random.seed(67)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r'inputs\\glass.csv')\n",
    "\n",
    "print(\"Glass Classification - Single Layer Perceptron (PyTorch)\")\n",
    "print(f\"\\nDataset: {df.shape[0]} samples, {df.shape[1]-1} features\")\n",
    "print(f\"\\nClass counts:\\n{df['Type'].value_counts().sort_index()}\")\n",
    "\n",
    "# Extract features and labels\n",
    "X = df.drop('Type', axis=1).values\n",
    "y = df['Type'].values\n",
    "\n",
    "# Remap labels to consecutive integers (0, 1, 2, ...)\n",
    "encode = LabelEncoder()\n",
    "y = encode.fit_transform(y)\n",
    "\n",
    "n_class = len(np.unique(y))\n",
    "print(f\"\\nFeatures: {X.shape}, Classes: {n_class}\")\n",
    "print(f\"Label mapping: {dict(zip(encode.classes_, range(len(encode.classes_))))}\")\n",
    "\n",
    "# Split and scale\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scale = StandardScaler()\n",
    "x_tr = scale.fit_transform(x_tr)\n",
    "x_ts = scale.transform(x_ts)\n",
    "\n",
    "print(f\"Train: {x_tr.shape[0]}, Test: {x_ts.shape[0]}\")\n",
    "\n",
    "# Convert to tensors\n",
    "x_tr_t = torch.FloatTensor(x_tr)\n",
    "y_tr_t = torch.LongTensor(y_tr)\n",
    "x_ts_t = torch.FloatTensor(x_ts)\n",
    "y_ts_t = torch.LongTensor(y_ts)\n",
    "\n",
    "# Data loaders\n",
    "batch = 16\n",
    "tr_data = TensorDataset(x_tr_t, y_tr_t)\n",
    "ts_data = TensorDataset(x_ts_t, y_ts_t)\n",
    "tr_load = DataLoader(tr_data, batch_size=batch, shuffle=True)\n",
    "ts_load = DataLoader(ts_data, batch_size=batch, shuffle=False)\n",
    "\n",
    "\n",
    "class SLP(nn.Module):\n",
    "    def __init__(self, n_feat, n_class):\n",
    "        super(SLP, self).__init__()\n",
    "        self.layer = nn.Linear(n_feat, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "n_feat = x_tr.shape[1]\n",
    "model = SLP(n_feat, n_class)\n",
    "criter = nn.CrossEntropyLoss()\n",
    "optim_fn = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"Architecture\")\n",
    "print(model)\n",
    "n_param = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nParams: {n_param}\")\n",
    "\n",
    "\n",
    "def train(model, tr_load, ts_load, criter, optim_fn, epochs=1000):\n",
    "    hist_loss = []\n",
    "    hist_tr = []\n",
    "    hist_ts = []\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        loss_sum = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in tr_load:\n",
    "            outputs = model(inputs)\n",
    "            loss = criter(outputs, labels)\n",
    "\n",
    "            optim_fn.zero_grad()\n",
    "            loss.backward()\n",
    "            optim_fn.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "        avg_loss = loss_sum / len(tr_load)\n",
    "        tr_acc = correct / total\n",
    "        hist_loss.append(avg_loss)\n",
    "        hist_tr.append(tr_acc)\n",
    "\n",
    "        model.eval()\n",
    "        ts_cor = 0\n",
    "        ts_tot = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in ts_load:\n",
    "                outputs = model(inputs)\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                ts_tot += labels.size(0)\n",
    "                ts_cor += (pred == labels).sum().item()\n",
    "\n",
    "        ts_acc = ts_cor / ts_tot\n",
    "        hist_ts.append(ts_acc)\n",
    "\n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1:4d} | Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Train: {tr_acc:.4f} | Test: {ts_acc:.4f}\")\n",
    "\n",
    "    return hist_loss, hist_tr, hist_ts\n",
    "\n",
    "\n",
    "# Train\n",
    "hist_loss, hist_tr, hist_ts = train(\n",
    "    model, tr_load, ts_load, criter, optim_fn, epochs=1000\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Results\")\n",
    "def get_pred(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labs in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return np.array(preds), np.array(labels)\n",
    "\n",
    "\n",
    "y_tr_p, y_tr_t = get_pred(model, tr_load)\n",
    "y_ts_p, y_ts_t = get_pred(model, ts_load)\n",
    "\n",
    "tr_acc = accuracy_score(y_tr_t, y_tr_p)\n",
    "ts_acc = accuracy_score(y_ts_t, y_ts_p)\n",
    "\n",
    "print(f\"\\nTrain acc: {tr_acc:.4f} ({tr_acc*100:.2f}%)\")\n",
    "print(f\"Test acc: {ts_acc:.4f} ({ts_acc*100:.2f}%)\")\n",
    "\n",
    "\n",
    "print(\"Report (Test)\")\n",
    "print(classification_report(y_ts_t, y_ts_p, target_names=[str(c) for c in encode.classes_]))\n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "cm = confusion_matrix(y_ts_t, y_ts_p)\n",
    "print(cm)\n",
    "\n",
    "# Plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(hist_loss)\n",
    "axes[0].set_title('Loss vs Epochs')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Cross-Entropy')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(hist_tr, label='Train')\n",
    "axes[1].plot(hist_ts, label='Test')\n",
    "axes[1].set_title('Accuracy vs Epochs')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'outputs\\metrics_pytorch.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "torch.save(model.state_dict(), r'outputs\\glass_model.pth')\n",
    "\n",
    "print(\"\\nGraph saved: metrics_pytorch.png\")\n",
    "print(\"Model saved: glass_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
